Wir haben also gelernt, dass der Psychologe Tyrell bei Zwiebelgeruch ein, sagen wir es jugendfrei, ein leidenschaftliches Bauchkribbeln bekommt. Ah ja, und dessen Kribbeln dürfte wenig mit Essen zu tun gehabt haben. Überhaupt, Essen schien eine Art Dauerbrenner zu sein in der letzten Vorlesung, womöglich war ich hungrig beim letzten Thema, Pavlovs Hund, der Futter bekam und irgendwann schon gespeichert hat, wenn er nur eine Glocke läutete und der Garcia-Effekt, also diese Geschmacks-Aversion, womit man gezeigt hat, dass im Funktionskreis Nahrungsaufnahme Übelkeit eher mit Geschmack assoziiert wird, sprich gelernt wird, als dass es mit audiovisuellen Informationen assoziiert würde. Da hatten wir Licht- und Klickgeräusche. Das geht ja sogar so weit, dass wir Immunreaktionen konditionieren können, wie zum Beispiel Asthmareaktionen, wenn jemand einen Nuss- oder eher Blütenstaub oder Hausstaub anschaut. Und über all dem schwegte von mir die provokante Frage, inwiefern wir uns eigentlich von der kalifornischen Meerschnecke unterscheiden würden. Wir hatten bisher immer nur zwei oder mehr Reize uns angeschaut, die durch gemeinsame Darbietung miteinander verbunden werden. Damit haben wir schon einiges an Erklärungspotenzial für unser Verhalten finden können. Aber die Tiere oder Menschen sind bei der klassischen Konditionierung vom letzten Thema immer passiv. Heute schauen wir uns an, wie wir aktives Verhalten konditionieren, sprich lernen, sprich verstärken können. So können wir dann auch das Lernen komplexerer, willkürlicher Reaktionen, wie zum Beispiel das Fahrradfahren oder Kunststücke, die Haustiere machen, erklären. Wir schauen uns heute in Lernen 2 die operante Konditionierung an und wie bestimmte Verhaltensweisen verstärkt und verschiedene Verhaltensweisen auch aneinandergereiht werden können. Hier noch mal ein kleiner Überblick von letzter Woche, was wir uns da angeschaut haben, die Definition des Lernens und vor allem die klassische Konditionierung, auch ein bisschen Habituation haben wir gehabt. Heute die operante Konditionierung und Verstärkerpläne. Letzte Woche hatten wir den little Albert, der uns sozusagen gefolgt ist, heute den kleinen Peter. Nächste Woche dann Lernen durch Einsicht, latentes Lernen, Beobachtungslernen. Hier noch mal zur Erinnerung für Sie, die 1-2-Folien sozusagen, gegebenenfalls als Spickzettel, brauchen Sie nicht noch mal eine andere Datei zu öffnen, Spickzettel auch zur klassischen Konditionierung und hier, was wir zum Gassier-Effekt hatten. Ja, vielleicht noch mal zur Erinnerung, zur Begriffsdefinition, eine kurze Erinnerung, Lernen haben wir definiert als Ergebnis von Erfahrungen, wobei der Prozess des Erfahrungsmachens zu konsistenten Änderungen des Verhaltenspotenzials führte. Verhaltenspotenzial, wir haben das Potenzial genannt deshalb, weil nicht unmittelbar Verhalten gezeigt werden muss, sondern das Lernen führt eben zu einer Anreicherung unseres Verhaltenrepertoires. Sozusagen können wir die Vielfalt, in der wir uns verhalten können, durch Lernen vergrößern. Schauen wir uns mal ein paar Beispiele von solchen, naja, was man halt so zum Beispiel Tieren beibringen kann, wo wir mit der klassischen Konditionierung eben an unsere Grenzen stoßen würden. Hier so ein Experiment mit Ratten, ich lasse es einfach mal einmal laufen. Wir sehen jetzt hier so ein Squirrel, so ein Eichhörnchen, das werden wir am Ende der Vorlesung sehen, wie es gelungen ist, dem so viele Kunststücke aneinanderzureihen. Der muss da irgendwo hochklettern, da rumspringen, sich an so einem Seil herunterhangeln. Tja, und das alles nur, um an Futter zu bekommen, das ist dann seine, das war dann seine Belohnung am Ende. Tja, was man nicht alles tut, um an Futter zu kommen, wie gesagt, das könnte man mit der klassischen Konditionierung, können wir das nicht mehr erklären. Übrigens, eigentlich habe ich bei YouTube nach der Ratte Barnebas gesucht, weil Pirell und Schirrmann 1963 das erklärt haben, das steht im Becker-Karus, dem Begleitwerk zu dieser Vorlesung, oder dem wichtigsten Begleitwerk zu dieser Vorlesung, was ich natürlich ja auch zur Vorbereitung benutze, wo eben die Ratte Barnebas zu so einem Varieté-Künstler ähnlich, wie wir es hier gesehen haben, trainiert wurde, aber die Enttäuschung war sehr groß. Sie hätte nicht größer sein können, als ich zwar ein vielversprechendes Video bei YouTube gefunden habe, was Ratte Barnebas hieß, war auch noch der erste Treffer, aber ich empfehle Ihnen, suchen Sie es nicht, begnügen Sie sich vielleicht mit dem Video, also das war ein sehr enttäuschendes Video. Ich habe noch ein zweites mitgebracht, und zwar wie man hinbekommen hat, dass Tauben Ping-Pong spielen können. Wir können mit Reinigung sehr komplexe Leistungen bilden, zum Beispiel eine Art Tauben-Ping-Pong. Endlich reinigen wir die Tauben nur, wenn sie die Balle über ihren Gegner erreichen, genauso wie im Menschenspiel. Das Ergebnis ist ein hart gebrachtes Wettbewerb. In diesen und vielen anderen Experimenten auf Behauptung werden moderne elektronische Geräte benutzt, um die Bedingungen zu arrangieren, wie sie sich auf die Puppe befinden. Das war wirklich ein Hardcore-Contest zwischen den beiden Tauben. Wenn wir diese Art von, ja, letztlich gar nicht mehr natürlichen Verhaltens, wie bei den Tauben gesehen, erklären wollen, kommen wir nicht um die Lernprinzipien von Thorndike, der das als erstes erklärt hat. Wenn wir diese Art von, ja, letztlich gar nicht mehr natürlichen Verhaltens, wie bei den Tauben gesehen, erklären wollen, kommen wir nicht um die Lernprinzipien von Thorndike, der das als erstes beschrieb. Edward Thorndike beschäftigte sich mit der Frage, wie im nicht reflexartiges Verhalten durch Erfahrungen modifiziert werden können. Und dafür arbeitete er mit hungrigen Tieren, die waren futterdepriviert, und die setzte er in einen solchen Problemkäfig und platzierte außerhalb dieses Käfigs, gut sichtbar für das Tier, eben Futter. Und jetzt ist es nun mal so, dass die Tiere, die tigern da dann auf und ab in diesem Käfig und zeigen ein gewisses Verhalten. Und meistens ist ja, also, Thorndike hat die Annahme, das Verhalten wäre an Konsequenzen ausgerichtet. Weil man eben etwas möchte, verhält man sich in einer bestimmten Art und Weise. Man ist also aktiv. Der Tier oder das Mensch ist in dieser Situation aktiv. Der Tier tut irgendwas in diesem Käfig und im Prinzip möchte es, weil es futterdepriviert ist, eben an das Essen draußen herankommen. Also, wie gesagt, das Tier tigert da irgendwie rum und was Thorndike gemacht hat in dem Problemkäfig ist, dass er unterschiedliche Reaktionen eben unterschiedlich belohnt hat. Zum Beispiel, wenn es auf eine Plattform tritt oder an einer Schnur zieht oder auf einen Hebel drückt. Das heißt, in so einem ersten Durchgang tut das Tier, wir haben glaube ich gerade im Bild davor, was war das, eine Katze gesehen, die gerne die Tür öffnen möchte, um da an das Essen heranzukommen. Und jetzt macht die Katze im ersten Durchgang irgendwas Zufälliges. Und was der Experimentator, was Thorndike dann machte, ist auf ein bestimmtes Verhalten eben die Tür zu öffnen, damit die Katze dort rausgehen kann. Und wenn man das mehrfach wiederholt, dass man immer auf ein bestimmtes Verhalten der Katze, wie gesagt, zum Beispiel, wenn es an einer Schnur zieht, dann diese Tür öffnet, dann stellt man fest, dass sich das an der Schnur ziehen, diese Reaktion, wiederholt, dass es häufiger wird. Deswegen steht hier mehrfache Wiederholung, die das Tier zum Öffnen des Käfigs benötigt. Und nach vielen Durchgängen ist die erste Handlung nach Betreten des Käfigs, das heißt, die hat Futterdepressur, geht in den Käfig und das allererste, was es macht, es zieht die Schnur, weil es eben durch Trial and Error im Prinzip gelernt hat, dass sich dadurch die Luke öffnet und es Futter bekommt. Die Lernprinzipien von Thorndike, das ist eben das Gesetz, das wichtigste Gesetz im Prinzip, was Thorndike dort festsetzt, ist das Gesetz des Effektes, the Law of Effect. Lernen wird durch Konsequenzen kontrolliert. Also die Kraft eines Stimulus, eine Reaktion hervorzurufen, wird verstärkt, wenn der Reaktion eine Belohnung folgt. Notziehen, Belohnung, Essen. Er formuliert auch noch ein anderes Gesetz, nämlich das Gesetz der Übung, Law of Exercise. Das besagt, dass die Wiederholung des gemeinsamen Auftretens von Reiz und Reaktion stärkt die Verknüpfung zwischen Reiz und Reaktion. Allerdings nur dann, wenn dem, also allerdings unterstützt nur eine verstärkte Wiederholung den Lernprozess. Also verstärkt hier die belohnte Reaktion. Und als drittes noch das Zugehörigkeitsprinzip, the Principle of Belongingness. Manche Reizreaktionen, Verknüpfungen lassen sich leichter herstellen als andere, also weil gewisse Reize mit gewissen Reaktionen natürlich assoziiert sind. Wir haben das auch schon gesehen bei dem Garcia-Effekt, dass es anscheinend so natürliche Assoziationen gibt, oder die eben leichter fallen. Wobei wir hier sagen müssen, dieses Zusammengehörigkeitsprinzip, Zugehörigkeitsprinzip, das akzeptierte Thorndike nur widerwillig, da darin irgendwie sowas Mentalistisches steckte, also das passte nicht zum behavioristischen Paradigma, was wir uns ja schon angeschaut haben, das gehe ich gleich nochmal ein. Also die Zusammengehörigkeit kann auch beim klassischen Konditionieren beobachtet werden, wie bei der Geschmackserversion, habe ich gerade schon gesagt. Aber da ist ja irgendwas da, was man eben nicht richtig beobachten kann, denn der Behaviorismus, der ist ja eigentlich eine Theorie davon, dass man alles beobachten müsste und alles, was man eben nicht beobachten kann, im Grunde nicht von Interesse ist. Skinners Theorie, Theorie der Verstärkung von solchen Verhalten, Behaviorist Skinner, er sagt, naja, die Mehrzahl der von Menschen gezeigten Verhaltensweisen, operanten Verhaltensweisen gehen eben auf klar definierbare Reize zurück. Übrigens ein Bild von Bohus F. Skinner, B. F. Skinner, das Typische, was man so kennt, ist dieses Bild. Ich war ganz erstaunt, auch mal ein jüngeres Bild von ihm zu sehen. Hier vielleicht auch ein Zitat mal erwähnt von ihm. Er sagte, manchmal werde ich gefragt, sehen Sie sich selbst auch als ein dieser Organismen, die Sie studieren? Darauf antwortete der Bohus F. Skinner. Ja, nach allem, was ich weiß, war mein Verhalten in jedem beliebigen Moment nichts anderes als das Produkt aus meiner genetischen Veranlagung, meiner persönlichen Geschichte und den jeweiligen Umständen. Skinner, Unterschied zwischen Respondenten- und Operantenverhalten. Respondent, das ist das, was wir in der klassischen Konditionierung gesehen haben, das Verhalten als eine Reaktion auf einen Reiz. Also, man könnte es auch abkürzen, Verhalten steht unter einer Reizkontrolle. Wir hatten das, dass man auf einem Stimulus, einem anderen Stimulus, dass die Glocke bei dem Hund läutet und er dann eben einspeichert. Operant, das, was wir uns heute anschauen, Operante-Konditionierung, das ist eben das Verhalten, das ist instrumentell, da ist eine Konsequenz. Konsequenz ist hier ein wichtiger Begriff, in der Umwelt herbeiführt. Und diese Konsequenz würde es ohne das Verhalten eben nicht geben. Die Katze würde ihr Futter nicht bekommen, wenn sie nicht an den Faden ziehen würde. Das heißt, bei der klassischen Konditionierung folgt auf einen Reiz eben ein anderer Reiz, und hier muss sich eine Reaktion zeigen, um dann eben Futter zu bekommen. Operantes Verhalten, das passiert sozusagen, ohne dass ein ankündigten Stimulus erkennbar wäre. Bewegung eines Säuglings zum Beispiel, herumlaufen in einem Zimmer, alles, was wir als freiwilliges Verhalten bezeichnen, wirken. Das heißt, bei operantem Verhalten wirkt, operiert, deswegen operant, der Organismus auf seine Umwelt. Man könnte das auch als Lernen am Erfolg beschreiben. Nochmal kurz zur Zusammenfassung, aus dem Becker-Karus kurz vorgelesen. Beim Lernen am Erfolg wird im Gegensatz zu pavlovschen klassischen Konditionierungen Lernen nicht als eine Assoziation zwischen zwei Reizen verstanden, sondern als eine Reiz-Reaktions-Assoziation, wobei diese Verbindung zwischen einem Stimulus und der ausgewählten Reaktion des Organismus durch Bekräftigung verstärkt wird. Das ist das Gesetz des Effekts. Aufbauend auf Thorndikes Auffassung, auf seinen Gesetzen sozusagen, hat Skinner ein Paradigma entwickelt und ausgearbeitet, um Verhalten so zu untersuchen und auch so zu steuern, wie wir es gerade mit den Tauben gesehen haben. Auch Skinner hatte eine solche Box, die sogenannte Skinner-Box, typischerweise für Ratten, auf denen ein Gitter zu sehen ist, wo sie einen Hebel drücken können und sie dann Futter bekommen. In manchen gibt es dann eben auch noch Lampen, die aufleuchten, und je nachdem, was die Ratte machen muss, manchmal ist auch dieses hier unten ein Licht, auch der Boden kann mit Strom versetzt werden, um dort bestimmte Reize zu setzen. Jedenfalls arbeitete Skinner hauptsächlich mit Ratten, mit Tieren, mit Tauben, und dieser Problemkäfig, den sehen wir hier, den ich hier meine, die Skinner-Box, sehr bekannt. Das ist also ein Käfig mit einem Futtermagazin, einem Trog, mit einer Futterausgabe, die durch bestimmtes Verhalten ausgelöst werden kann, und diese Lampe hier kann eben vom Versuchsleiter gesteuert werden, und auch das Bodengitter kann elektrifiziert werden. Kommen wir zum Operanten-Konditionieren. Skinners Arbeit beruht letztlich auf so einem Hedonismus-Ansatz, dass er sagt, naja, Verhalten ist nun mal, ist auf Steigerung von Lust und Vermeidung von Schmerz ausgerichtet. Das ist eine Annahme. Operantes Konditionieren beschreibt nun, die Wahrscheinlichkeit eines Verhaltens wird dadurch erhöht, dass dem Verhalten eine Verstärkung folgt, dass ich das Verhalten irgendwie belohne. Dadurch erhöht sich die Wahrscheinlichkeit, bestimmtes Verhalten zu zeigen. Und eine Verstärkung ist ein Reiz, der nach mehrmaliger Darbietung, kontingent zu einer Reaktion, die Auftretenswahrscheinlichkeit dieser Reaktion beeinflusst. Und Skinner meinte damit jeden Reiz, der sich objektiv beschreibt. Zum Beispiel, wenn man die Aufmerksamkeit von jemandem bekommt, dann kann man das objektiv beschreiben. Genauso aber auch Geld oder Lob und Klopfen auf die Schultern. All dies muss aber unabhängig, wir sind ja im Behaviorismus, unabhängig von möglichen internen Prozessen sein, sondern man muss es objektiv sehen können, welcher Reiz eben dazu diente, etwas zu verändern. Welcher Reiz eben dazu diente, etwas zu verstärken, sprich die Auftretenswahrscheinlichkeit zu erhöhen. Schauen wir uns hier die Tabelle an. So ein Stimulus, eine mögliche Reaktion, und die Konsequenz. Zum Beispiel, VP sieht einen Reiz auf dem Bildschirm, irgendwas blinkt auf, wie auch immer, sie führt eine Reaktion aus, Stimulus, Response, und die Konsequenz ist dann eben, hält, eine Rückmeldung. Die hungrige Ratte ist in der Skinnerbox, drückt den Hebel und kriegt Futter. Ein kleiner Junge fällt hin, weint und kriegt die Aufmerksamkeit. Wir können die operante Konditionierung im Grunde in vier Phasen einteilen. Also wenn man so ein Experiment zum Beispiel mit diesen Tauben macht. Am Anfang beobachte ich die Basisrate, also wenn ich so eine Taube oder so eine Ratte in die Box stecke, dann schaue ich mir zunächst erstmal an, ohne dass ich irgendwas konditioniere, ohne dass ich irgendwas verstärke, schaue ich mir an, ja, wie häufig drückt denn so eine Ratte jetzt diesen Hebel? Das ist ja ein zufälliges Verhalten, das tut sie vielleicht auch einfach so. Dann, in der zweiten Phase, verstärke ich das Verhalten. Das ist die Trainingsphase, wo ich eben gezielt das Verhalten stärke, das eben häufiger gezeigt werden soll. Da steht dann hier, man belohnt es, sozusagen, dass dieses Verhalten, zum Beispiel drücken auf den Hebel, wird dann erhöht. In der dritten Phase, wir kennen die Begriffe auch schon aus der klassischen Konditionierung, Löschung des Verhaltens, das Verhalten wird nicht weiter belohnt. Ratte drückt auf den Hebel, aber es passiert nichts, kriegt kein Futter. Dann sieht man, das Verhalten geht wieder auf ein Ausgangsniveau zurück. Und auch hier finden wir wieder den Begriff der Spontanerholung, dass eben nach dieser Löschungsphase das Verhalten unter Umständen erneut auftritt. Hier mal dargestellt in Grafiken, hier Anzahl der Reaktionen pro Zeit. Das ist eine Basisrate. Ich zähle hier einfach, wie häufig drückt jetzt die Ratte auf den Hebel drauf. Das ist die Basisrate über die Zeit. Immer mal drückt das da drauf. Dann, hier unten wäre sozusagen, wie häufig ich das Experiment mache, bzw. wie häufig ich die Ratte in den Käfig setze. Beim ersten Mal in den Käfig zeigt es so und so viele Reaktionen. Und meinetwegen beim zehnten Mal, wie ich es in den Käfig setze, drückt sie schon in dem fiktiven Beispiel jetzt eben 150 Mal auf den Hebel. Löschungsphase, Extinktionsphase, die Wahrscheinlichkeit, auf den Hebel zu drücken, geht eben zurück. Und hier die Spontanerholung, zur Erinnerung, das ist eine Folie vom letzten Mal, von der klassischen Konditionierung, wo wir genau das Gleiche auch gesehen haben, wo dann hier die Spontanerholung stattfindet. Ich habe hier mal nur einen Punkt, oder als Erweiterung oder Korrektur vom letzten Mal, wenn das hier nur mal so ein Punkt ist, diese Spontanerholung kann kommen, wenn ich einfach noch mal diesen Condition Stimulus gebe, oder auch wenn ich es noch mal trainiere. Also diese Phasen, wenn ich hier diese Spontanerholung habe, und dann geht es wieder runter, dann kann eben diese spontane Erholung wirklich spontan, ohne dass irgendwas passiert ist, oder weil ich es eben einmalig noch mal trainiere, also Futter noch mal gebe, wenn Sie den Hebel drücken. Das ist im Prinzip hier aneigene Löschung in drei verschiedenen Versuchungen zur Verdeutlichung der Spontanerholung, die dann hier noch mal wieder abnimmt, die Reaktion noch mal abnimmt. Also noch einmal zur letzten Klarstellung, weil ich selbst im Gespräch mit meinen Kollegen da nicht hundert Prozent einig war, was wir so unter Spontanerholung verstehen, oder eben nicht. Deswegen hier für uns noch mal zur Definition, spontane Erholung, oder spontane Remission, ist eben das, wenn Paflowscher Hund hat eigentlich schon lange hat Glocke gehört, lange Zeit keinen Speichel mehr gegeben, weil man ihm kein Futter mehr gegeben hat. Er hat also verlernt, auf die Glocke einzuspeicheln. Und jetzt hört er diese Glocke und spontan speichelt er wieder, als hätte man ihn trainiert. Dann haben wir hier plötzlich, obwohl sein Niveau vom Speichel irgendwo hier liegt, geben wir ihm nach langer Zeit einfach mal eine Glocke, es klingelt an der Tür, wie auch immer, und plötzlich speichelt er wieder ganz doll. Spontanerholung. Spontanerholung kann auch dann passieren, wenn ich nach einer Ruhephase wieder anfange zu trainieren. Ich gebe ihm eben Glocke und Futter und stelle fest, ich fange hier oben irgendwo an, habe eine Spontanerholung, viel stärker, als er eigentlich speicheln würde, wenn er es wirklich komplett vergessen hätte. Und wenn ich dann weiter trainiere, fange ich dann eben ab dem Punkt an. Dann würde die Kurve so gehen, wie mein Laserpointer hier gerade. Dann fange ich also schon auf einem höheren Niveau an und erreiche also das maximale Speicheln sozusagen frühzeitiger. So das nochmal als Erklärung. Becker Karus steht es genauso. Kommen wir zu einem Beispiel wieder zurück zur Operantenkonditionierung.
