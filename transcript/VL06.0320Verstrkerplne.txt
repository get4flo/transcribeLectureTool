Bisher sind wir davon ausgegangen, dass wir jede gewünschte Reaktion in dem Moment, oder wenn sie denn auftritt, auch verstärken. Das wäre so eine sogenannte kontinuierliche Verstärkung. Jede operante Reaktion wird verstärkt. Man muss aber nicht jede Reaktion direkt verstärken. Deswegen spricht man, also wenn man da unterschiedliche Herangehensweisen hat, was man verstärkt und wann man verstärkt, redet man von sogenannten Verstärkerplänen. Die schauen wir uns mal an. Es gibt zum Beispiel auch die intermittierende Verstärkung. Da wird man nicht jede operante Reaktion verstärken. Man unterscheidet in feste Pläne und Variable Pläne, sowie Quotenpläne und Intervallpläne. Dann hat man hier so eine 2x2-Tapel. Ein fester Quotenplan heißt, eine Verstärkung erfolgt nach bestimmter Anzahl von Reaktionen. Ein variabler Quotenplan heißt, eine Verstärkung erfolgt nach variabler, aber unvorhersagbarer Anzahl von Reaktionen. Ein fester Intervallplan sagt, es wird die erste Reaktion nach einer bestimmten Zeitdauer der letzten Verstärkung verstärkt. Und der Variable Intervallplan heißt dann, wenn man verstärkt, wenn man die erste Reaktion nach einem variablen Zeitintervall verstärkt, wobei die durchschnittliche Dauer festgelegt ist. Ein fester Quotenplan ist ein Stück Lohn. Ich mache eine bestimmte Anzahl, kriege ich einen Lohn für. Variabler Quotenplan sind Gewinnspielautomaten. Es ist unvorhersehbar, wann was ausgeschüttet wird. Und es wird eben nach einer bestimmten Anzahl an Reaktionen und nicht nach einer bestimmten Zeit etwas ausgespuckt, also ein Gewinn ausgespuckt. Ein fester Intervallplan ist das Gehalt. Ich kriege nach einer festen und vorhersehbaren Zeit eben Geld. Und variable Intervallpläne, wo es vielleicht durchschnittlich im Jahr sechsmal Blumen gibt, aber manchmal auch zweimal in der Woche hintereinander werden, so Blumen vom Partner oder die Partnerin. In der Grafik wird es ein bisschen deutlicher. Oder können wir uns ein bisschen besser anschauen, was diese Pläne ausmacht. Wir sehen also hier die Häufigkeit der Reaktionen, die gezeigt werden, und hier die Zeit, die verstreicht. Der fixierte Quotenplan. Hier sehen wir immer, hier wird was eingeübt, hier wird was gegeben, und dann sehen wir die Reaktion sozusagen darauf, wenn was gegeben wurde. Bei einem festen Quotenplan haben wir hohe Reaktionsraten und mit mehr Reaktionen haben wir auch mehr Verstärkung. Beim variablen Quotenplan haben wir hohe Reaktionsraten, da mehr Reaktionen auch mehr Verstärkung zeigt. Das gleiche also. Beim festen Intervallplan steigert sich das Verhalten kurz vor erwarteten Belohnungszeitpunkt. Danach hat man meistens so ein bisschen abgeflacht, so ein gewisses Null-Niveau. Das heißt, man ahnt schon, ich kriege bald eine Belohnung und dann zeige ich die Reaktion eben häufiger. Ich glaube, da hatten wir als Beispiel das Gehalt. Ob das da so gut klappt, dass man zum Ende des Monats, wenn das Gehalt kommt, tatsächlich mehr arbeitet, da bin ich ganz so sicher. Und bei einem variablen Intervallplan, da hat man nicht so hohe Reaktionsraten. Das Erlernen dauert länger, aber dafür sehr stetig und man kann ja hier keine Vorhersage machen, wann kommt die nächste Belohnung, aber sehr löschungsresistent, wenn man auf diese Art und Weise verstärkt hat. Schauen wir uns auf der nächsten an. Denn die Wirkung der verschiedenen Verstärkungspläne lässt sich anhand dreier Merkmale vergleichen, nämlich der Schnelligkeit des Erwerbs. Da stellen wir fest, Quotenpläne sind besser als Intervallpläne. Quotenpläne, man lernt schneller als in den Intervallplänen. Die Löschungswiderstand, der ist bei Variablen viel besser als Feste. Also in den Variablenplänen sind die Löschungen nicht ganz so stark, weil es variabler ist, wann ich die Belohnung bekomme. Dadurch kann man eben auch mal eine etwas längere Zeit sozusagen überbrücken, ohne dass das Verhalten unbedingt belohnt werden muss. Und der Rhythmus des Verhaltens. Da ist eben bei Variablen, also wenn man ein sehr gleichmäßiges Verhalten haben möchte, dann sehen wir hier, sind die Variablenpläne besser als die fixen oder die festen Pläne, wo es eben immer mal so ein bisschen schwankt. Bei Variablen schwankt es dann eben weniger. Ja, Verstärkerpläne, können wir ein Fazit ziehen. Kontinuierlich Verstärkung führt schneller zum angestrebten Verhalten als Intermittierende Verstärkung. Intermittierende Verstärkung führt zu einer höheren Löschungsresistenz als kontinuierliche Verstärkung. Quotenpläne führen im Allgemeinen zu einer höheren Reaktionshäufigkeit als Intervallpläne. Intervallpläne führen im Allgemeinen zu einem langsameren Anstieg der Reaktionshäufigkeit als Quotenpläne. Variable Pläne führen zu einem über die Zeit gesehen sehr gleichmäßigen Verhalten und fixierte Pläne führen zu stark wechselnden Verhaltenshäufigkeiten. Ich könnte das jetzt noch fünfmal runterbeten. Schauen Sie sich die die Fazit noch mal anhand der Grafik an, um die ein bisschen besser zu verinnerlichen. Machen wir ein Beispiel. Ein Kind klettert zum ersten Mal alleine auf den Stuhl. Jetzt wird es gelobt von den Eltern. Das ist eine Verstärkung. Zuerst loben die Eltern das Kind jedes Mal, wenn es alleine auf den Stuhl klettert. Das ist dann eine kontinuierliche Verstärkung. Später loben die Eltern das Verhalten nur noch ab und zu und irgendwann schon gar nicht mehr. Das heißt, wir haben hier eine variable, intermittierende Verstärkung. Das Ausbleiben des Verstärkers hat dann eben keine Konsequenzen mehr für das Kind. Aber durch den erfolgreichen Lernprozess gibt es nun andere Verstärker. Zum Beispiel kann das Kind nun, weil es hochgeklettert ist, sich mit den Dingen auf dem Tisch beschäftigen, was eben wieder verstärkt werden kann. Der optimale Verstärkungsplan ist zunächst eine kontinuierliche Verstärkung zum Verhaltensaufbau und dann gefolgt von einer über einen längeren Zeitraum abnehmenden, intermittierenden Verstärkung und variablen Verstärkung zur Verhaltensstabilisierung, so wie wir es hier eben gesehen haben. Welche zeitlichen Faktoren spielen eine Rolle? Wie wirkt sich die Verstärkungsverzögerung denn nun auf die Lerngeschwindigkeit aus? In Grice 1948 haben wir zum Beispiel so einen Versuchsaufbau gehabt. Ratten liefen von einer Startkammer durch so eine Diskriminationskammer zu einer Zielkammer. Und hier erfolgt eine Verstärkung, wenn in der Diskriminationskammer die richtige Wahl getroffen wurde, also eine Art Labyrinth, wo sie dann eben in eine bestimmte Richtung gehen konnten. Und zwischen der Diskriminationskammer und der Zielkammer war ein grauer Gang einfach, und die Länge dieses Gangs hat man variiert. Wie gesagt, in der Zielkammer gab es dann den Verstärker. Römern hat jetzt 20 Durchgänge pro Ratte pro Tag gemacht, und je nach Gruppe betrug die Verzögerung der Verstärkung, ja also wie lang der Tunnel war, hat man also mehrere Gruppen für die Ratten gemacht, betrug dann eben dieser Gang 0 Sekunden, eine halbe Sekunde, 1,2 Sekunden, 2 Sekunden, 5 Sekunden oder 10 Sekunden, je nachdem wie lang der eben war. Wenn sie die richtige Wahl getroffen hatten, sind sie eben in den Gang gekommen, der sie zur Verstärkung führte. Im Ergebnis stellt man fest, dass je größer die Verstärkungsverzögerung war, desto langsamer wurde das Verhalten auch gelernt. Hier sehen wir die Kammer, die sehr weit erst weg war. Da haben sie, wenn die Belohnung sozusagen, wenn der Gang sehr lang war, die Belohnung 10 Sekunden auf sich gewartet hat, dann haben sie sehr lang gebraucht, bzw. gar nicht, wenn ich das hier richtig interpretiere, diese Grafik, am Ende gar nicht gelernt, ob sie eben die linke Tür oder rechte Tür benutzen sollen. Ja, weil wenn man zwei Sachen zur Auswahl hat, na ja, dann ist das ja hier gerade die Ratewahrscheinlichkeit. Ich muss jetzt aber gestehen, dass ich nicht weiß, wie viele Türen sie zur Auswahl hatten, die eben zur Belohnung für die Ratten führten. Ja, ein anderer Punkt, wo man sich bestärkt fühlt, ist auch das abergläubische Verhalten, wo es eben eine Verstärkung für nicht-kontingentes Verhalten gibt. Verstärkung wird unabhängig vom Verhalten gegeben. Es erhöht sich die Auftretenswahrscheinlichkeit desjenigen Verhaltens, das zufällig vor der Stärkung gezeigt wurde. Da hat auch Skinner schon ein Aberglaubenexperiment gemacht, und zwar lief das so ab, dass da Tauben in der Skinnerbox waren, und die haben alle 15 Sekunden, unabhängig von dem, was sie vorher gemacht wurden, eine Belohnung bekommen. Einfach so, alle 15 Sekunden Belohnung bekommen. Und jetzt stellt ihr fest, dass nach einiger Zeit die meisten Tiere bestimmte Verhaltensweisen zeigten, die sie inzwischen in Verstärkergabe irgendwie wiederholten. Zum Beispiel drehten sie sich irgendwie im Kreis, oder zeigten bestimmte ruckartige Bewegungen, pickten auf einer bestimmten Stelle, plusterten sich auf. Also, wir sehen, abergläubisches Verhalten entsteht meist, wenn Lebewesen keine Kontrolle über die Ereignisse haben, über die Verstärker haben. Und es zeigte sich auch sehr löschungsresistent. Ein anderes, und da kommen wir zum Anfang der aktuellen Vorlesung, ist der Punkt des Shapings. Verhalten kann sehr variabel sein, und diese Variabilität des Verhaltens kann dafür genutzt werden, um noch nie zuvor gezeigte Verhaltensweisen zu trainieren. Wir haben diese Eichhörnchen gesehen, die eben so einen Parcours abliefen. Das erreicht man dadurch, dass man eben bestimmtes Verhalten, man sagt Shaped, dass man durch sukzessive Annäherung eben das Verhalten bestärkt, was man am Ende gern sehen möchte. Wir versuchen, das Verhalten dem gewünschten Verhalten schrittweise anzunähern. So soll zum Beispiel eine Ratte lernen, einen Hebel bis zum Anschlag herunterzudrücken. Eine richtige Herkules-Ratte wollen wir sozusagen erzeugen. Und das können wir machen, wir setzen die Ratte wieder in ihre Box, und zuerst verstärken wir vielleicht, dass sie sich überhaupt diesen Hebel hinwendet, dann dass sie den berührt, dann verstärken wir, wenn sie es leicht berührt, und dann machen wir den Widerstand dieses Hebels immer schwerer, und sukzessive erhöhen wir sozusagen das Gewicht, was die Ratte herunterdrücken muss, und irgendwann wird sie so ein richtig schweres, was hatten wir hier, Mekakarus steht ein Beispiel, von der Ratte Herkules, die 250 Gramm wog, und am Ende sogar einen Hebel mit 512 Gramm drücken konnte. Ja, und wenn eine Reaktion nun erlernt wurde, können wir diese auch wieder mit anderen Reaktionen verknüpfen. Das ist das, was wir am Anfang mit den Eichhörnchen gesehen haben, die wir dann sogar, wo wir verschiedene Reaktionen aneinander gereiht haben, was dann Chaining genannt wird. Schauen wir uns das hier einmal an, wenn es den angeht. Ich weiß gar nicht, ob es einen Ton hat. Hier wird also belohnt, dass sie sich überhaupt diesem Hebel annähert. Ja, allein die Nähe dazu. Jetzt das Schnüffeln sozusagen belohnt, das Aufrichten sozusagen. Jetzt muss ich den Hebel schon berühren, damit sie eine Belohnung bekommt. Jetzt muss ich den drücken. Man hat es ganz klein bisschen gesehen, dass es sich bewegt. Also der grüne Punkt hier hieß, dass da dann Futter rauskommt. Ich wundere, das Video hatte keinen Ton. Ich wollte trotzdem Sicherheit sein und meinen Ohrstöpsel im Ohr haben, um zu hören, ob nicht doch da ein Ton kommt. Ja, Anwendung. Wie gesagt, in der Tierdressur kann das benutzt werden, aber eben auch in der Verhaltenstherapie, um irgendwie Neuartiges oder eben nicht gezeigtes Verhalten zu verstärken. Ein Tier soll lernen, den richtigen Knopf des Fernsehapparats zu drücken, oder Menschen mit Schizophrenie oder Autismus sollen ein angemessenes soziales und verbales Verhalten zeigen. Ja, das Shaping kann mit verschiedenen Verstärkerplänen und Chaining kombiniert werden. Ja, zum Beispiel mit so einem variablen Quotenplan. Eine Reaktion wird nur dann verstärkt, wenn sie besser ist als eine vorangegangene oder als die Quote der vorangegangenen Reaktionen. Und mit zunehmender Dauer des Trainings wird eben nur noch das Verhalten verstärkt, das immer besser mit dem Verhaltensziel übereinstimmt. Zum Beispiel, wenn man mit dem Rauchen aufhören soll. Zunächst gibt es eine Belohnung, wenn man eine angebotene Zigarette nicht annimmt. Wenn das irgendwann leicht fällt, dann wird die Quote erhöht, und es gibt nur noch eine Belohnung, wenn man zweimal eine angebotene Zigarette ablehnt. Fällt dies leicht, wird die Quote wiederum erhöht, und wenn man das dreimal hintereinander schafft, naja, Sie wissen, worauf ich hinaus möchte. Auch hier gibt es biologische Einschränkungen. Das Lernpotenzial eines Organismus ist beschränkt durch seine genetische Ausstattung bezüglich Wahrnehmung, Verhalten, Kognition natürlich. Und die Genetik bestimmt bestimmte Verhaltens- und Umweltbeziehungen. Und da gibt es zum Beispiel die Punkte des Autoshaping und der Instinktverschiebung. Instinktverschiebung, Breland & Breland 1961, das waren Tiertrainer, die bei Skinner gelernt hatten. Und die beobachteten immer wieder, dass bestimmtes konditionierte operante Verhalten scheitert. Beispiel, ein Waschbär und die zweite Münze. Ein Waschbär wurde mühevoll darauf konditioniert, eine Münze in eine Spardose zu stecken. So, das hat er erstmal hingekriegt irgendwann. Allerdings brach dieses konditionierte Verhalten, eine Münze in eine Dose zu stecken, dann zusammen, wenn der Bär eine zweite Münze hatte. Dann, wenn der Bär zwei Münzen bekommt, dann reihte er lieber diese zweite Münze an die erste an. Und das entspricht viel eher seinem natürlichen, angeborenen Verhaltensrepertoire. Waschbären reiben wohl gerne, reiben und waschen gerne so Sachen, vor allen Dingen Flusskrebse, das sind ihre Lieblingsnahrung. Und also sie beschäftigen sich viel damit, so Flusskrebse von ihren Schalen zu machen, wofür sie dann eben reiben. Das, was er mit dieser Münze dann gemacht hat. Also Instinktverschiebung ist, mit zunehmender Erfahrung wird das verstärkte Verhalten durch instinktive Verhaltensweisen abgelöst, die der Organismus in seiner natürlichen Umgebung auf der Suche nach dem Verstärker, nach dem Futter zeigt. Es ist eine, wie sagten sie es, es ist eine Instinktneigung, dass sich gelerntes Verhalten mit der Zeit dem instinktiven Verhalten annähert. Also das gelernte Verhalten eigentlich, eine Münze in eine Dose zu stecken, nähert sich wieder dem instinktiven Verhalten eben an, Sachen aneinander zu reiben. Der zweite Punkt, den ich sagte, war Autoshaping von Brown & Jenkins 1968. Sie hatten das Ziel zu testen, ob Tauben auf eine Taste picken, obwohl dies für die Futtergabe nicht nötig ist. Sie hatten eine Trainingsphase, da gaben sie eine Reaktionstaste oder die Reaktionstaste wurde in unregelmäßigen Abstanden mit weißem Licht beleuchtet und danach kam das Futter. Und obwohl für die Futtergabe keine Reaktion nötig war, fingen die Tauben an, auf die gewünschte Taste zu picken. Das kann über die klassische Konditionierung erklärt werden, im Prinzip der Stimulus-Substitution. Hauder, auch ein Beispiel des pavloschen Huns, der lernte vielleicht nicht die Verbindung Klingel-Speichel, sondern höchstwahrscheinlich wurde das Futter nur durch die Klingel ersetzt. Ja, und so eine Taube pickt nun mal, wenn sie Essen bekommt. Nicht alle Verhaltensweisen unterliegen der Kontrolle von Verstärkern. Genetisch programmierte Verhaltensweisen können oft eben nicht durch Verstärker ausgestaltet werden. Ja, das sind dann biologische Einschränkungen. Die durch Autoshaping entstandenen Verhaltensweisen sind Konditionierungen eines Systems von Verhaltensweisen, die mit der Belohnung im Unconditional Stimulus zusammenhängen. Es gehen aus der Erklärung dazu ist, dass die Verstärkung nicht die einzige Determinante ist, die auf das Verhalten wirkt. Bei Instinktverschiebung zum Beispiel wirken die phylogenetischen und ondrogenetischen Einflüsse gleichzeitig auf das Verhalten. Wie gesagt, nun mal puckt, pickt jetzt so eine Taube nun mal, wenn sie Futter bekommt. Die Vererbung konkurriert mit der Kontingenz der Verstärkung. Verhaltensweisen können also nur in Abhängigkeit der genetisch programmierten Reaktionen verändert werden, wenn man so möchte. Ja, und das bringt uns zur Zusammenfassung. Der Verbindung von angeborenem und erlerntem Verhalten kommt eine entscheidende Bedeutung zu. Durch klassische Konditionierung erhalten zuvor neutrale Reize Kontrolle über Reflexverhalten. Durch operante Konditionierung kann die Auftretenswahrscheinlichkeit von Verhaltensweisen in Abhängigkeit ihrer Konsequenzen verändert werden. Und durch Shaping und Chaining können neue, komplexe Verhaltensweisen gelernt werden. Konditionierung ist dann effektiv, wenn das Zielverhalten biologisch relevant ist. Wenn es zum Beispiel um Essen geht, so wie hier in dem abschließenden Comic dieser zwei Ratten. Beim nächsten Mal beschäftigen wir uns dann mit noch komplexeren Lernverhalten und gewinnen eine kognitive Sicht auf das Lernen und verabschieden uns da aus dem BWL-Biorismus. Bis dahin.
